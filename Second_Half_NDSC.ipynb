{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jztrs1XjL_B0"
   },
   "source": [
    "# EDA Version 0.2\n",
    "In this approach, we will divide the dataset into 3 Big categories, and then we train each category alone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "path = './Data/'\n",
    "fil = os.path.join(path, 'train.csv')\n",
    "csvfilename = open(fil, 'r').readlines()\n",
    "file = 1\n",
    "splitsize = 200000\n",
    "for j in range(len(csvfilename)):\n",
    "    if j % splitsize == 0:\n",
    "        open(str(fil)+ str(file) + '.csv', 'w+').writelines(csvfilename[j:j+splitsize])\n",
    "        file += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_csv.reader' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f7f1cd21f261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-83560abd48bd>\u001b[0m in \u001b[0;36msplit\u001b[1;34m(filehandler, delimiter, row_limit, output_name_template, output_path, keep_headers)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcurrent_limit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_limit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeep_headers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mcurrent_out_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_csv.reader' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "path = './Data'\n",
    "filepath = os.path.join(path, 'train.csv')\n",
    "split(open(filepath, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f83c6d50081b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkeras_lib\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m   model_to_estimator = tf_export('keras.estimator.model_to_estimator')(\n\u001b[0;32m     30\u001b[0m       keras_lib.model_to_estimator)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFinalExporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLatestExporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_to_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_fn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEstimatorSpec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\inputs.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy_input_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_input_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\numpy_io.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueues\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeeding_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\queues\\feeding_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m   \u001b[0mHAS_PANDAS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     from pandas._libs import (hashtable as _hashtable,\n\u001b[0m\u001b[0;32m     27\u001b[0m                              \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                              tslib as _tslib)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m from .tslibs import (\n\u001b[0m\u001b[0;32m      5\u001b[0m     iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime, Period)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_convert_single\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\conversion.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\timedeltas.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.timedeltas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\offsets.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.offsets\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\ccalendar.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.ccalendar\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.TimeRE.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.TimeRE.__seqToRE\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pytz\\lazy.py\u001b[0m in \u001b[0;36m_lazy\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_iter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                         \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_props\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                             \u001b[0mdelattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLazyList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1080\u001b[0m  'Zulu']\n\u001b[0;32m   1081\u001b[0m all_timezones = LazyList(\n\u001b[1;32m-> 1082\u001b[1;33m         tz for tz in all_timezones if resource_exists(tz))\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[0mall_timezones_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLazySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_timezones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36mresource_exists\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;34m\"\"\"Return true if the given resource exists\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mopen_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36mopen_resource\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     95\u001b[0m         filename = os.path.join(os.path.dirname(__file__),\n\u001b[0;32m     96\u001b[0m                                 'zoneinfo', *name_parts)\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m             \u001b[1;31m# http://bugs.launchpad.net/bugs/383171 - we avoid using this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;31m# unless absolutely necessary to help when a broken version of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqneb_yvJsHZ"
   },
   "source": [
    "## 1- Importing libraries\n",
    "\n",
    "also doing some logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Knyl3IOjFl1p"
   },
   "outputs": [],
   "source": [
    "#Load all the needed libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uhlq6Z3AEbg"
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "h76BVT_T8TBx",
    "outputId": "dbbceb7b-6dec-4c0c-8d74-1caa2de9aa2a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrMAW2IJJ44P"
   },
   "source": [
    "## 2- Loading Train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJ2wb-eG87FK"
   },
   "outputs": [],
   "source": [
    "#path = '/content/drive/My Drive/Colab Notebooks/ndsc/'\n",
    "path = './Data'\n",
    "dftrain = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "dftest = pd.read_csv(os.path.join(path, 'test.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path,'categories.json')) as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "pprint(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCDKIHFJKfGW"
   },
   "source": [
    "## 3- Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMAZArjwKBmt"
   },
   "source": [
    "### 3.1 Divide the training data into Big categories containers (Beauty/Fashion/Mobile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbPM_2lX9KeC"
   },
   "outputs": [],
   "source": [
    "# 1. add a new col that contins the big category (beauty, fashion, mobile)\n",
    "\n",
    "newCol = dftrain.image_path.apply(lambda x : x.split('_')[0])\n",
    "dftrain['mainCat'] = newCol\n",
    "# dftrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dftrain['mainCat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwHS6FuAKRd2"
   },
   "source": [
    "### 3.2 Display the total number of samples under each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "LN2eHDj49xcE",
    "outputId": "53102ccb-73d5-4614-c93e-e9804a258564"
   },
   "outputs": [],
   "source": [
    "Beautydf= dftrain[dftrain['mainCat'] == 'beauty']\n",
    "Fashiondf= dftrain[dftrain['mainCat'] == 'fashion']\n",
    "Mobiledf= dftrain[dftrain['mainCat'] == 'mobile']\n",
    "\n",
    "plt.bar(['beauty', 'fashion', 'mobile'],[Beautydf.shape[0],Fashiondf.shape[0],Mobiledf.shape[0]],width=0.2, color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmwUhgE5DqvY"
   },
   "source": [
    "#### 3.2.1 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "vTMye7hy-FJP",
    "outputId": "b0567249-2fca-44ee-d0b7-cd686ced5725"
   },
   "outputs": [],
   "source": [
    "Fashiondf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fRXttglcJKfM",
    "outputId": "8a8329ba-0da1-42e9-8842-802f30578732"
   },
   "outputs": [],
   "source": [
    "Fashiondf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKBIlRuYLCvw"
   },
   "source": [
    "### 3.3 Randomize the train-dataset\n",
    "\n",
    "To avoid bias in the training dataset, it is a good practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beautydf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "-E7rZ5DABFv1",
    "outputId": "4905ac78-e71f-4ec9-bd8b-01f9caf6faea"
   },
   "outputs": [],
   "source": [
    "# randomize datasets\n",
    "\n",
    "Beautydf = Beautydf.reindex(np.random.permutation(Beautydf.index))\n",
    "Fashiondf = Fashiondf.reindex(np.random.permutation(Fashiondf.index))\n",
    "Mobiledf = Mobiledf.reindex(np.random.permutation(Mobiledf.index))\n",
    "\n",
    "# Beautydf.Category.hist(width = 0.5)\n",
    "# Fashiondf.Category.hist(width = 0.5)\n",
    "# Mobiledf.Category.hist(width = 0.5)\n",
    "\n",
    "fig, catAx = plt.subplots(1, 1)\n",
    "\n",
    "catAx.hist(Beautydf.Category, label='Beauty', width=0.5)\n",
    "catAx.hist(Fashiondf.Category, label='Fashion', width=0.5)\n",
    "catAx.hist(Mobiledf.Category , label='Mobile', width=0.5)\n",
    "\n",
    "catAx.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_wvv61oLR9d"
   },
   "source": [
    "### 3.4 Remove any number with more than 3 digits in the dataset\n",
    "\n",
    "For example : `02854323232` which could be an id or something\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "Omco1mHbA0ay",
    "outputId": "fb7b91a9-95b9-4b36-fdf7-2bf9abe3353c"
   },
   "outputs": [],
   "source": [
    "# Remove any number in the dataset with more than or equal to 3 digits. \n",
    "\n",
    "Beautydf.title.replace(r'\\b[0-9]{3,}\\b', '', regex=True)\n",
    "Fashiondf.title.replace(r'\\b[0-9]{3,}\\b', '', regex=True)\n",
    "Mobiledf.title.replace(r'\\b[0-9]{3,}\\b', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qYlNXNTBVnN"
   },
   "source": [
    "### 3.5 Doing further analysis on the Fashion category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fashiondf['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fashiondf['Category'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Check the phrase of the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fashiondf['title_len'] = [len(t) for t in Fashiondf['title']]\n",
    "Fashiondf['title_wc'] = [len(t.split()) for t in Fashiondf['title']]\n",
    "Fashiondf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the distribution looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"Category\", y=\"title_len\",\n",
    " #           hue=\"smoker\", palette=[\"m\", \"g\"],\n",
    "            data=Fashiondf)\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"Category\", y=\"title_wc\",\n",
    " #           hue=\"smoker\", palette=[\"m\", \"g\"],\n",
    "            data=Fashiondf)\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the boxplot there's no clear differentiation based on the word count or the length of the title. But there appears to be some outliers on Category 26 Blouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Analyzing the count vectors next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fs_vect = CountVectorizer(ngram_range=(1,6), stop_words=stopwords)\n",
    "cvector = CountVectorizer(ngram_range=(1,6))\n",
    "cvector.fit(Fashiondf.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvector.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3,149,454 words have been extracted from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTermFrequency(df, category, cat_title):\n",
    "    '''Given a dataframe and a category, return term frequency'''\n",
    "    matrix = cvector.transform(df[df.Category == category].title)\n",
    "    words = matrix.sum(axis=0)\n",
    "    words_freq = [(word, words[0, idx]) \n",
    "                           for word, idx in cvector.vocabulary_.items()]\n",
    "    tf = pd.DataFrame(list(sorted(words_freq,key = lambda x: x[1], \n",
    "                            reverse=True)),columns=['Terms',cat_title])\n",
    "    tf_df = tf.set_index('Terms')\n",
    "    return tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Building a word cloud for each category in Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordCloud(df, category):\n",
    "    ''' Generate a word cloud given a dataframe with the respective category'''\n",
    "    new_df = df[df.Category == category]\n",
    "    new_words = []\n",
    "    for t in new_df.title:\n",
    "        new_words.append(t)\n",
    "    new_text = pd.Series(new_words).str.cat(sep=' ')\n",
    "    \n",
    "    wordcloud = WordCloud(width=1600, height=800, \n",
    "                      max_font_size=200).generate(new_text)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_dict = {}\n",
    "for k,v in (categories['Fashion'].items()):\n",
    "    print (\"Generating the word cloud for category {}\".format(k))\n",
    "    generateWordCloud(Fashiondf, v)\n",
    "    tf = getTermFrequency(Fashiondf, v, k)\n",
    "    term_freq_dict[v] = tf\n",
    "    print (tf.head(10))\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df = pd.concat(term_freq_dict, axis =1)\n",
    "term_freq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Remove the top common words across all categories which may not be helping with the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGJGU8K-LrFu"
   },
   "source": [
    "## 4- Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2s_QXTvL71M"
   },
   "source": [
    "### 4.1 Split the training dataset into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78ITO1LDGDQ5"
   },
   "outputs": [],
   "source": [
    "B_X_train = Beautydf.title\n",
    "B_y_train = Beautydf.Category\n",
    "\n",
    "F_X_train = Fashiondf.title\n",
    "F_y_train = Fashiondf.Category\n",
    "\n",
    "M_X_train = Mobiledf.title\n",
    "M_y_train = Mobiledf.Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(vect, X_train, X_test):\n",
    "    ''' Tokenize the training and test set'''\n",
    "    # create document-term matrices using the vectorizer\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "    # print the number of features that were generated\n",
    "    #print('Features: ', X_train_dtm.shape[1])\n",
    "    \n",
    "    return X_train_dtm, X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skfolds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "for train_index, test_index in skfolds.split(M_X_train, M_y_train):\n",
    "    y_train_folds = M_y_train.iloc[train_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "import sklearn.metrics as metrics\n",
    "import sys\n",
    "\n",
    "def trainCV(X_train, y_train, vect, folds = 5, random_state=42, \n",
    "            clf=SGDClassifier(random_state=42, max_iter=5000, tol=1e-3, \n",
    "                              loss='hinge', verbose=1)):\n",
    "    ''' Train a classifier using CV'''\n",
    "    #clf = SGDClassifier(random_state=random_state)\n",
    "\n",
    "    skfolds = StratifiedKFold(n_splits=folds, random_state=random_state)\n",
    "\n",
    "    cv_start_time = time.monotonic()\n",
    "    \n",
    "    scores = []\n",
    "    features = []\n",
    "    for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "        start_time = time.monotonic()\n",
    "        \n",
    "        clone_clf = clone(clf)\n",
    "        #X_train_folds = X_train[train_index]\n",
    "        y_train_folds = y_train.iloc[train_index]\n",
    "        #X_test_fold = X_train[test_index]\n",
    "        y_test_fold = y_train.iloc[test_index]\n",
    "        \n",
    "        X_train_folds,X_test_fold = tokenize_data(vect, \n",
    "                                                  X_train.iloc[train_index], \n",
    "                                                  X_train.iloc[test_index])\n",
    "        features.append(X_train_folds.shape[1])\n",
    "        \n",
    "        clone_clf.fit(X_train_folds, y_train_folds)\n",
    "        y_pred = clone_clf.predict(X_test_fold)\n",
    "        n_correct = sum(y_pred == y_test_fold)\n",
    "        scores.append(n_correct / len(y_pred))\n",
    "        \n",
    "        end_time = time.monotonic()\n",
    "        print(\"Training time per fold: {}\".format(timedelta(seconds=end_time - start_time)))\n",
    "\n",
    "        \n",
    "    print (\"Features: {}\".format(features))\n",
    "    print(\"Features variance: %0.2f (+/- %0.2f)\" % (np.mean(features), \n",
    "                                                    np.std(features) * 2))\n",
    "    print (\"Scores: {}\".format(scores))\n",
    "    print(\"Accuracy: %0.5f (+/- %0.5f)\" % (np.mean(scores), \n",
    "                                           np.std(scores) * 2))\n",
    "    \n",
    "    cv_end_time = time.monotonic()\n",
    "    print(\"Total Training time: {}\".format(timedelta(seconds=cv_end_time - cv_start_time)))\n",
    "    \n",
    "    return clone_clf, scores, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Defining Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Bahaya Stop words \n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "id_stopwords = factory.get_stop_words()\n",
    "en_stopwords  = list(stopwords.words('english'))\n",
    "stopwords = id_stopwords + en_stopwords\n",
    "stopwords[1:20]\n",
    "\n",
    "#F_stopWords = ['dress', 'lengan', 'wanita', 'women', 'untuk', 'neck', 'model', 'top', 'panjang']\n",
    "#F_stopWords2 = ['lengan', 'untuk', 'neck', 'model', 'top', 'panjang']\n",
    "#F_stopWords3 = ['untuk', 'neck', 'model', 'top', 'panjang']\n",
    "#B_stopWords = ['lip', 'cream', 'promo', 'bb', 'original', 'cushion', 'ml', 'natural', 'nature', 'murah']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Training for Beauty\n",
    "\n",
    "#### 4.3.1 Optiming for nrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal ngram\n",
    "means = []\n",
    "for i in range(2,15):\n",
    "    vect = CountVectorizer(ngram_range=(1,i))\n",
    "    print (\"Ngram hyperparam: {}\".format(i))\n",
    "    B_sgd, scores, features = trainCV(B_X_train, B_y_train, vect)\n",
    "    means.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(range(2,15))\n",
    "plt.xlabel('Ngram range')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Optimal Nrange param for Beauty')\n",
    "plt.plot(label, means);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal hyperparam is at nrange 1,8 with Accuracy: 0.78536 (+/- 0.00335)\n",
    "\n",
    "With slight improvement seen using One vs Rest with Accuracy: 0.78557 (+/- 0.00376)\n",
    "\n",
    "And with stop words Accuracy: 0.78435 (+/- 0.00327)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Train the optimal model with SGD\n",
    "Default with max iter = 20, loss = hinge, toll = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_clf = OneVsRestClassifier(SGDClassifier(random_state=42, \n",
    "                                            max_iter=5000, \n",
    "                                            tol=1e-3,loss='hinge'))\n",
    "ovr_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_vect = CountVectorizer(ngram_range=(1,8))\n",
    "Bs_vect = CountVectorizer(ngram_range=(1,8), stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using One vs One with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_sgd, B_scores, B_features = trainCV(B_X_train, B_y_train, Bs_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one vs rest with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Br_sgd, Br_scores, Br_features = trainCV(B_X_train, B_y_train, Bs_vect,\n",
    "                                         clf=ovr_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OvO without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bo_sgd, Bo_scores, Bo_features = trainCV(B_X_train, B_y_train, B_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Using other model for Beauty\n",
    "\n",
    "Using logistic regression to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,8))\n",
    "clf = LogisticRegression(random_state=42, solver='lbfgs', \n",
    "                         multi_class='multinomial')\n",
    "B_logr, scores, features = trainCV(B_X_train, B_y_train, vect, clf=clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using multinomial nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,8))\n",
    "clf = MultinomialNB()\n",
    "B_nb, scores, features = trainCV(B_X_train, B_y_train, vect, clf=clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random forest to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "vect = CountVectorizer(ngram_range=(1,8))\n",
    "B_rf, scores, features = trainCV(B_X_train, B_y_train, vect, clf=forest_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model Training for Mobile\n",
    "\n",
    "#### 4.4.1 Optimizing for the best nrange for Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_means = []\n",
    "for i in range(2,15):\n",
    "    vect = CountVectorizer(ngram_range=(1,i))\n",
    "    print (\"Ngram hyperparam: {}\".format(i))\n",
    "    M_sgd, scores, features = trainCV(M_X_train, M_y_train, vect)\n",
    "    M_means.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(range(2,15))\n",
    "plt.xlabel('Ngram range')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Optimal Nrange param for Mobile')\n",
    "plt.plot(label, M_means);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal hyperparam for Mobile is with nrange of 1,5 with Accuracy: 0.82828 (+/- 0.00265)\n",
    "\n",
    "Using One vs Rest is seeing slight improvement with Accuracy: 0.82856 (+/- 0.00143).\n",
    "\n",
    "And with stop words: Accuracy: 0.82006 (+/- 0.00239)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Training for Optimal Model for Mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Retrain using One vs rest as it seems to be outperforming One vs One classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms_vect = CountVectorizer(ngram_range=(1,5), stop_words=stopwords)\n",
    "M_vect = CountVectorizer(ngram_range=(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OvO with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mo_sgd, Mo_scores, Mo_features = trainCV(M_X_train, M_y_train, Ms_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OvR with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mr_sgd, Mr_scores, Mr_features = trainCV(M_X_train, M_y_train, Ms_vect, \n",
    "                                        clf=ovr_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OvO without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_sgd, M_scores, M_features = trainCV(M_X_train, M_y_train, M_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model Training for Fashion\n",
    "\n",
    "#### 4.5.1 Optiming for the nrange for Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_vect = CountVectorizer(ngram_range=(1,6), analyzer='word')\n",
    "Fs_vect = CountVectorizer(ngram_range=(1,6), stop_words=stopwords, analyzer='word')\n",
    "Fss_vect = CountVectorizer(ngram_range=(1,6), stop_words=stopwords, \n",
    "                           analyzer='word', tokenizer=textblob_tokenizer,\n",
    "                          min=1, max=0.6, max_features=1e5)\n",
    "\n",
    "F_sgd = OneVsRestClassifier(SGDClassifier(random_state=42, \n",
    "                                            max_iter=5000, \n",
    "                                            tol=1e-3,\n",
    "                                            loss='hinge',\n",
    "                                            alpha=1e-4,\n",
    "                                            penalty='l2',\n",
    "                                            verbose =0\n",
    "                                         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_means = {}\n",
    "scores = {}\n",
    "features = {}\n",
    "for i in range(1,7):\n",
    "    for j in range(2,7):\n",
    "        if i <= j:\n",
    "            vect = CountVectorizer(ngram_range=(i,j), stop_words=stopwords, analyzer = 'word')\n",
    "            print (\"Ngram hyperparam: {},{}\".format(i,j))\n",
    "            F_sgd, scores[i], features[i] = trainCV(F_X_train, F_y_train, vect, clf=F_sgd)\n",
    "            F_means[\"{}_{}\".format(i,j)] = np.mean(scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for i in range(1,7):\n",
    "    for j in range(2,7):\n",
    "        if i <= j:\n",
    "            label = list(range(i,j))\n",
    "            plt.xlabel('Ngram range')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Optimal Nrange param for Fashion')\n",
    "            plt.plot(label, F_means[\"{}_{}\".format(i,j)]);\n",
    "            k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal for Fashion is Ngram range of 1,14 with Accuracy: 0.64612 (+/- 0.00261)\n",
    "\n",
    "And using One vs Rest with Accuracy: 0.64587 (+/- 0.00192).\n",
    "\n",
    "With stop words with Accuracy: 0.64569 (+/- 0.00502).\n",
    "\n",
    "#### 4.5.2 Find Optimal Model for Fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OvO with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fo_sgd, Fo_scores, Fo_features = trainCV(F_X_train, F_y_train, Fs_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OvR with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fr_sgd, Fr_scores, Fr_features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                         clf=F_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ovr with stopwords and stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fr_sgd, Fr_scores, Fr_features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                         clf=F_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OvO without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_sgd, F_scores, F_features = trainCV(F_X_train, F_y_train, F_vect, clf=F_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Manual tuning to optimize SGD\n",
    "\n",
    "Comparing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScores(func, params, scores):\n",
    "    '''Plot the scores'''\n",
    "    plt.title(\"Effect of {}\".format(func))\n",
    "    plt.xlabel(func)\n",
    "    plt.ylabel(\"score\")\n",
    "    x = np.arange(len(params))\n",
    "    plt.xticks(x, params)\n",
    "    plt.plot(x, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "losses = [\"hinge\", \"log\", \"modified_huber\", \"perceptron\", \"squared_hinge\"]\n",
    "l_scores = []\n",
    "\n",
    "for loss in losses:\n",
    "    print (\"Training using loss: {}\".format(loss))\n",
    "    model = OneVsRestClassifier(SGDClassifier(random_state=42, \n",
    "                                            max_iter=5000, \n",
    "                                            tol=1e-3,loss=loss))\n",
    "    F_model, scores, features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                        clf=model)\n",
    "    l_scores.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotScores(\"loss\", losses, l_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = [\"l2\", \"l1\", \"none\"]\n",
    "p_scores = []\n",
    "\n",
    "for p in penalty:\n",
    "    print (\"Training using penalty: {}\".format(p))\n",
    "    model = OneVsRestClassifier(SGDClassifier(random_state=42, \n",
    "                                            max_iter=5000, \n",
    "                                            tol=1e-3,penalty=p))\n",
    "    F_model, scores, features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                        clf=model)\n",
    "    p_scores.append(np.mean(scores))\n",
    "\n",
    "plotScores(\"penalty\", penalty, p_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.0001, 0.001, 0.01, 0.1]\n",
    "a_scores = []\n",
    "\n",
    "for a in alpha:\n",
    "    print (\"Training using alpha: {}\".format(a))\n",
    "    model = OneVsRestClassifier(SGDClassifier(random_state=42, \n",
    "                                            max_iter=5000, \n",
    "                                            tol=1e-3,alpha=a))\n",
    "    F_model, scores, features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                        clf=model)\n",
    "    a_scores.append(np.mean(scores))\n",
    "\n",
    "plotScores(\"alpha\", alpha, a_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above param to fine tune the model\n",
    "* alpha: 0.0001 Accuracy: 0.65286 (+/- 0.00186)\n",
    "* penalty: l2 Accuracy: 0.65286 (+/- 0.00186)\n",
    "* loss: hinge Accuracy: 0.65333 (+/- 0.00224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Optimize the count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0.01, 0.1, 0.01):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = [1, 10, 100, 1000, 10000]\n",
    "min_scores = []\n",
    "\n",
    "for m in min_df:\n",
    "    print (\"Training using min df: {}\".format(m))\n",
    "    vect = CountVectorizer(ngram_range=(1,6), stop_words=stopwords, analyzer='word',\n",
    "                                  min_df=m, max_df=1.0, max_features=None)\n",
    "\n",
    "    F_model, scores, features = trainCV(F_X_train, F_y_train, vect, \n",
    "                                        clf=F_sgd)\n",
    "    min_scores.append(np.mean(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotScores(\"min df\", min_df, min_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = [0.4, 0.6, 0.8, 1.0]\n",
    "max_scores = []\n",
    "\n",
    "for m in max_df:\n",
    "    print (\"Training using max df: {}\".format(m))\n",
    "    vect = CountVectorizer(ngram_range=(1,6), stop_words=stopwords, analyzer='word',\n",
    "                                  min_df=1, max_df=m, max_features=100000)\n",
    "\n",
    "    F_model, scores, features = trainCV(F_X_train, F_y_train, vect, \n",
    "                                        clf=F_sgd)\n",
    "    max_scores.append(np.mean(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotScores(\"max df\", max_df, max_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feat = [10000, 100000, 250000, 500000, None]\n",
    "mf_scores = []\n",
    "\n",
    "for m in max_feat:\n",
    "    print (\"Training using max features: {}\".format(m))\n",
    "    vect = CountVectorizer(ngram_range=(1,6), stop_words=stopwords, analyzer='word',\n",
    "                                  min_df=1e-4, max_df=1.0, max_features=m)\n",
    "\n",
    "    F_model, scores, features = trainCV(F_X_train, F_y_train, vect, \n",
    "                                        clf=F_sgd)\n",
    "    mf_scores.append(np.mean(scores))\n",
    "\n",
    "plotScores(\"max df\", max_feat, mf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best count vectorizer param appears to be at min df =1, max df = 0.6 and max features at 100,000.\n",
    "\n",
    "#### 4.5.5 Testing stemming next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Use TextBlob\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words\n",
    "\n",
    "# Use NLTK's PorterStemmer\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Grid search to optimize the SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "}\n",
    "\n",
    "model = SGDClassifier(max_iter=5000, tol=1e-3)\n",
    "clf = GridSearchCV(model, param_grid=params, cv=5,error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_X_train_dtm = tokenize_data(F_vect, F_X_train, F_X_train)\n",
    "clf.fit(F_X_train_dtm, F_y_train)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.5 Manual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "wedding23 = ['wedding','pernikahan','pesta pernikahan', 'gaun']\n",
    "shirt27 = ['kemeja','shirt','polos']\n",
    "casual18 = ['motif','musim','musim panas']\n",
    "maxi20 = ['maxi']\n",
    "bigsize24 = ['ukuran','ukuran besar']\n",
    "bodycon22 = ['bodycon','mini', 'sexy']\n",
    "party19 = ['pesta','party','lace','brokat','brukat']\n",
    "blouse26 = ['blouse','blus']\n",
    "tshirt25 = ['kaos', 'kaos shirt', 'pendek', 'lengan pendek']\n",
    "croptop29 = ['crop','crop top']\n",
    "tanktop28 = ['tank top','tank', 'top']\n",
    "others17 = ['baju']\n",
    "aline21 = ['bahan']\n",
    "bigsizetop30 = ['jumbo','modal longgar','big size','ada ukuran']\n",
    "\n",
    "def getFashionCategory(X):\n",
    "        if any(elem in X.split() for elem in wedding23):\n",
    "            return 23\n",
    "        elif any(elem in X.split() for elem in shirt27):\n",
    "            return 27\n",
    "        elif any(elem in X.split() for elem in casual18):\n",
    "            return 18\n",
    "        elif any(elem in X.split() for elem in maxi20):\n",
    "            return 20\n",
    "        elif any(elem in X.split() for elem in bigsize24):\n",
    "            return 24\n",
    "        elif any(elem in X.split() for elem in bodycon22):\n",
    "            return 22\n",
    "        elif any(elem in X.split() for elem in party19):\n",
    "            return 19\n",
    "        elif any(elem in X.split() for elem in blouse26):\n",
    "            return 26\n",
    "        elif any(elem in X.split() for elem in tshirt25):\n",
    "            return 25\n",
    "        elif any(elem in X.split() for elem in croptop29):\n",
    "            return 29\n",
    "        elif any(elem in X.split() for elem in tanktop28):\n",
    "            return 28\n",
    "        elif any(elem in X.split() for elem in others17):\n",
    "            return 17\n",
    "        elif any(elem in X.split() for elem in aline21):\n",
    "            return 21\n",
    "        elif any(elem in X.split() for elem in bigsizetop30):\n",
    "            return 30\n",
    "        else:\n",
    "            return 18 # most common category\n",
    "\n",
    "class FashionClassifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return X.apply(getFashionCategory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_X_train.head().apply(getFashionCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "F_clf = FashionClassifier()\n",
    "cross_val_score(F_clf, F_X_train, F_y_train, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.6 Other Models\n",
    "\n",
    "Using Tfid vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,6), stop_words=stopwords, analyzer = 'word') #CountVectorizer(ngram_range=(1,14), stop_words=F_stopWords2, analyzer = 'word')\n",
    "Fsovr_sgd, scores, features = trainCV(F_X_train, F_y_train, vect, clf=F_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Log regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', max_iter=5000, C=1, solver='lbfgs',\n",
    "                        multi_class=\"multinomial\")\n",
    "F_logr, scores, features = trainCV(F_X_train, F_y_train, vect, clf=clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC(kernel='rbf') \n",
    "F_svm, F_scores, F_features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                         clf=svm_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt_clf = tree.DecisionTreeClassifier()\n",
    "F_dt, F_scores, F_features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                         clf=dt_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_clf = MultinomialNB(alpha = 0.07)\n",
    "F_nb, F_scores, F_features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                         clf=nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=100, max_features=F_X_train.shape[0], max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=10,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=1,\n",
    "            warm_start=False)\n",
    "F_nb, F_scores, F_features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                         clf=rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xg_clf = XGBClassifier(verbosity=2,tree_method='gpu_exact')\n",
    "F_nb, F_scores, F_features = trainCV(F_X_train, F_y_train, Fs_vect, \n",
    "                                         clf=xg_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Checking the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0W0lO17Qyin"
   },
   "source": [
    "#### 4.6.1 Summary:\n",
    "---\n",
    " \n",
    "\n",
    "1.   **Beauty Category**:    `ngram_range(1, 8) OVR`\n",
    "---\n",
    "2.   **Fashion Category**:    `ngram_range(1, 14) OVR`\n",
    "---\n",
    "3.   **Mobile Category**:    `ngram_range(1,5) OVR`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = Bo_scores+ Fo_scores+ Mo_scores\n",
    "print(\"OvO with stopwords Accuracy: %0.5f (+/- %0.5f)\" % (np.mean(total_scores), \n",
    "                                       np.std(total_scores) * 2))\n",
    "\n",
    "total_scores = B_scores+ F_scores+ M_scores\n",
    "print(\"OvO without stopwords Accuracy: %0.5f (+/- %0.5f)\" % (np.mean(total_scores), \n",
    "                                       np.std(total_scores) * 2))\n",
    "\n",
    "total_scores = Br_scores+ Fr_scores+ Mr_scores\n",
    "print(\"OvR with stopwords Accuracy: %0.5f (+/- %0.5f)\" % (np.mean(total_scores), \n",
    "                                       np.std(total_scores) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Result comparison:\n",
    "* OvO with stopwords Accuracy: 0.75610 (+/- 0.15001)\n",
    "* OvO without stopwords Accuracy: 0.75598 (+/- 0.15262)\n",
    "* OvR with stopwords Accuracy: 0.75615 (+/- 0.14967)\n",
    "\n",
    "Your submission scored 0.76247, which is not an improvement of your best score. Keep trying!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "haSx19EUSbId"
   },
   "source": [
    "## 5- Modelling\n",
    "\n",
    "1. Divide the test dataset into 3 Categories\n",
    "2. Remove numbers more than 3 Digits from the test dataset.\n",
    "3. Train + Predict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnMVtuLnTUyg"
   },
   "source": [
    "### 5.1 Divide the test dataset into 3 Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70V6GmZGSeyO"
   },
   "outputs": [],
   "source": [
    "#add category col\n",
    "newTestCol = dftest.image_path.apply(lambda x: x.split('_')[0])\n",
    "dftest['newCat'] = newTestCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "bvq_8KSCTeD_",
    "outputId": "88e9f191-9749-4d67-a0a2-eebcb61ddf74"
   },
   "outputs": [],
   "source": [
    "dftest.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "ntxgP7EAWiXE",
    "outputId": "bb494828-7f91-47f3-a2d0-283875e0b996"
   },
   "outputs": [],
   "source": [
    "MobileTest = dftest[dftest['newCat'] == 'mobile']\n",
    "BeautyTest = dftest[dftest['newCat'] == 'beauty']\n",
    "FashionTest = dftest[dftest['newCat'] == 'fashion']\n",
    "\n",
    "fig, axt = plt.subplots(1,1)\n",
    "axt.bar(['beauty', 'fashion', 'mobile'], [len(BeautyTest), len(FashionTest), len(MobileTest)], width=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sumGyDEdFCe"
   },
   "source": [
    "### 5.3 Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyRvw_4cdKCe"
   },
   "outputs": [],
   "source": [
    "B_X_test = BeautyTest.title\n",
    "F_X_test = FashionTest.title\n",
    "M_X_test = MobileTest.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdlKUH1Bdhyg"
   },
   "source": [
    "### 5.4 Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelPredict(X_train, y_train, X_test, vect, \n",
    "                 clf=SGDClassifier(random_state=42,max_iter=5000, \n",
    "                                            tol=1e-3,loss='hinge')):\n",
    "    ''' Train a classifier using CV'''\n",
    "    #clf = SGDClassifier(random_state=random_state)\n",
    "    \n",
    "    X_train_dtm,X_test_dtm = tokenize_data(vect, X_train, X_test)\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_y_pred = modelPredict(B_X_train, B_y_train, B_X_test, Bs_vect, clf=ovr_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_y_pred = modelPredict(F_X_train, F_y_train, F_X_test, Fs_vect, clf=ovr_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_y_pred = modelPredict(M_X_train, M_y_train, M_X_test, Ms_vect, clf=ovr_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "swwqXL5sfgkR"
   },
   "source": [
    "### 5.5 Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "h8BuX46Vf-BF",
    "outputId": "c7563b58-db60-4801-b29f-4794bd3387ad"
   },
   "outputs": [],
   "source": [
    "BeautyTest['Category'] = B_y_pred\n",
    "FashionTest['Category'] = F_y_pred\n",
    "MobileTest['Category'] = M_y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bNvhS8XjX3j"
   },
   "source": [
    "## 6- Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "tHGPfI54gQRa",
    "outputId": "76035a87-e38d-46dd-d1fa-848fdada9b29"
   },
   "outputs": [],
   "source": [
    "resDf = BeautyTest.append(FashionTest).append(MobileTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4SdtRA4pjQs7"
   },
   "outputs": [],
   "source": [
    "FinalResult = pd.concat([resDf['itemid'], resDf['Category']], axis=1, \n",
    "                        keys=['itemid', 'Category'])\n",
    "FinalResult.to_csv(path + '{}_NDSC_Res.csv'.format(datetime.datetime.today().strftime('%Y%m%d')\n",
    "                                                  ), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGzSnqY6jrhG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Second_Approach NDSC",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
